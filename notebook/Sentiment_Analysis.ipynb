{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8uXXFc-cjLK",
        "outputId": "8f0c9f75-9335-40bf-af4f-0a665cff29a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m458.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement GoogleSearch (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for GoogleSearch\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain_community\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.12)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.3 marshmallow-3.23.0 mypy-extensions-1.0.0 pydantic-settings-2.6.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain.schema (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain.schema\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.12)\n",
            "Collecting openai<2.0.0,>=1.52.0 (from langchain_openai)\n",
            "  Downloading openai-1.52.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.52.0->langchain_openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain_openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain_openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.12->langchain_openai) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.12->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.12->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.12->langchain_openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.52.2-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed jiter-0.6.1 langchain_openai-0.2.3 openai-1.52.2 tiktoken-0.8.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.3.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.3 ffmpy-0.4.0 gradio-5.3.0 gradio-client-1.4.2 huggingface-hub-0.26.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.0 semantic-version-2.10.0 starlette-0.41.0 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for the script\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install openai==0.28\n",
        "!pip install GoogleSearch serpapi google-search-results\n",
        "!pip install langchain_community\n",
        "!pip install langchain.schema\n",
        "!pip install langchain_openai\n",
        "!pip install gradio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f9ZLnUmchjL"
      },
      "outputs": [],
      "source": [
        "# Libraries Import\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Import necessary sklearn modules for machine learning\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Import LangChain modules for building language model chains and agents\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate, SerpAPIWrapper\n",
        "from langchain.agents import (AgentExecutor, AgentType, LLMSingleActionAgent, Tool, initialize_agent, load_tools)\n",
        "from langchain.callbacks.manager import (AsyncCallbackManagerForToolRun, CallbackManagerForToolRun)\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.utilities import (GoogleSearchAPIWrapper, GoogleSerperAPIWrapper, WikipediaAPIWrapper)\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "\n",
        "# Import logging module for logging errors and other messages\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBjQivc-haVb",
        "outputId": "76f4b028-18e4-4ae8-a766-15de175f2bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-372d154f0c4e>:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm_35=ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "open_ai_key=os.environ[\"OPENAI_API_KEY\"]\n",
        "serper_api_key=os.environ[\"SERPER_API_KEY\"]\n",
        "llm_35=ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
        "llm_4 = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OteZTZbi-ct"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary of reliable sources with their respective reliability scores\n",
        "RELIABLE_SOURCES = {\n",
        "    \"accion-ciudadana.org\": 5.00,\n",
        "    \"tracoda.info\": 5.00,\n",
        "    \"elconfidencial.com\": 4.67,         # Spanish digital newspaper\n",
        "    \"elpais.com\": 4.67,                 # Renowned Spanish newspaper\n",
        "    \"eldiario.es\": 4.33,                # Independent Spanish media\n",
        "    \"abc.es\": 4.00,                     # Spanish generalist newspaper\n",
        "    \"europapress.es\": 4.00,             # Spanish news agency\n",
        "    \"lavanguardia.com\": 4.33,           # Catalonia-based newspaper\n",
        "    \"20minutos.es\": 4.00,               # Spanish daily news site\n",
        "    \"elespanol.com\": 4.00,              # Spanish digital newspaper\n",
        "    \"huffingtonpost.es\": 4.00,          # Spanish edition of the HuffPost\n",
        "    \"publico.es\": 4.00,                 # Spanish progressive newspaper\n",
        "    \"larazon.es\": 4.00,                 # Spanish daily newspaper\n",
        "    \"eldiarioar.com\": 4.33,             # Argentine edition of eldiario.es\n",
        "    \"cnnespanol.cnn.com\": 4.00,         # CNN's Spanish language site\n",
        "    \"bbc.com/mundo\": 4.00,              # BBC Spanish edition\n",
        "    \"rtve.es\": 4.33,                    # Spain's public broadcasting service\n",
        "    \"efe.com\": 4.00,                    # Spanish news agency\n",
        "    \"clarin.com\": 4.00,                 # Major Argentine newspaper\n",
        "    \"infobae.com\": 4.00,                # Latin American news site\n",
        "    \"pagina12.com.ar\": 4.00,            # Argentine newspaper\n",
        "    \"lavozdegalicia.es\": 4.00,          # Regional newspaper from Galicia, Spain\n",
        "    \"laopinion.com\": 4.00,              # US-based Spanish language newspaper\n",
        "    \"elespectador.com\": 4.00,           # Colombian newspaper\n",
        "    \"semana.com\": 4.00,                 # Colombian weekly magazine\n",
        "    \"elmundo.es\": 4.00,                 # Major Spanish newspaper\n",
        "    \"nacion.com\": 4.00,                 # Costa Rican newspaper\n",
        "    \"elpais.com.uy\": 4.00,              # Uruguayan edition of El Pais\n",
        "    \"eltiempo.com\": 4.00,               # Colombian newspaper\n",
        "    \"elpais.com.mx\": 4.00,              # Mexican edition of El Pais\n",
        "    \"eluniversal.com.mx\": 4.00,         # Major Mexican newspaper\n",
        "    \"laopiniondemurcia.es\": 4.00,       # Regional newspaper from Murcia, Spain\n",
        "    \"es.wikipedia.org\":5.00,\n",
        "    \"bloomberglinea.com\":1.00,\n",
        "    \"latimes.com\":1.00\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract information using the Serper API\n",
        "def extract_base_domain(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "    if domain.startswith(\"www.\"):\n",
        "        domain = domain[4:]\n",
        "    return domain\n",
        "\n",
        "def info_extraction(subject, length=500, api_key=serper_api_key, min_search=30):\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = json.dumps({\"q\": subject, \"gl\": \"sv\", \"hl\": \"es-419\", \"num\": min_search, \"tbs\": \"qdr:m\"})\n",
        "    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "        organic_results = results.get(\"organic\", [])\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error during search: {e}\")\n",
        "        return [], [], []\n",
        "\n",
        "    all_media_outlets, reliable_media, unreliable_media = [], [], []\n",
        "\n",
        "    for result in organic_results:\n",
        "        snippet = result.get('snippet')\n",
        "        source_url = result.get('link')\n",
        "        if snippet and source_url:\n",
        "            base_domain = extract_base_domain(source_url)\n",
        "            all_media_outlets.append(base_domain)\n",
        "\n",
        "            if base_domain in RELIABLE_SOURCES:\n",
        "                reliability_index = RELIABLE_SOURCES[base_domain]\n",
        "                reliable_media.append({\n",
        "                    \"snippet\": snippet[:length],\n",
        "                    \"source\": base_domain,\n",
        "                    \"reliability_index\": reliability_index\n",
        "                })\n",
        "            else:\n",
        "                unreliable_media.append({\"snippet\": snippet[:length], \"source\": base_domain})\n",
        "        else:\n",
        "            print(f\"Missing data in results: {result}\")\n",
        "\n",
        "    sorted_reliable = sorted(reliable_media, key=lambda x: x['reliability_index'], reverse=True)\n",
        "    return all_media_outlets, sorted_reliable, unreliable_media"
      ],
      "metadata": {
        "id": "rP6z6ETWwvDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOlexmgCz8YS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Base class for agents\n",
        "class WrapperFrame_v1:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "# Narrative Agents\n",
        "\n",
        "# 1. Positive Sentiment Narratives\n",
        "class ES_Agent_Positive(WrapperFrame_v1):\n",
        "    def __init__(self, client):\n",
        "        super().__init__(client)\n",
        "\n",
        "        self.ES_agent = \"\"\"\n",
        "    Your task is to analyze and categorize a social media post {input_text} from El salvador according to predefined narrative themes that reflect positive sentiment with validated context.\n",
        "\n",
        "\n",
        "    Narratives to Detect:\n",
        "\n",
        "      ESP1: The Government’s Firm Approach Restoring Law and Order.\n",
        "      Example: \"The government's hardline security policies have restored law and order in El Salvador, reducing crime rates and bringing a sense of safety to local communities.\"\n",
        "      Keywords: law and order, security, crime reduction, safety, firm approach.\n",
        "\n",
        "      ESP2: El Salvador is Becoming a Model of Security and Strong Leadership in Fighting Gang Violence.\n",
        "          Example: \"El Salvador's leadership in combating gang violence is becoming a model for other countries in the region, showcasing the government's resolve and effectiveness.\"\n",
        "          Keywords: security model, leadership, gang violence, regional example, safety.\n",
        "\n",
        "      ESP3: Government Policies Attracting Support and International Investment.\n",
        "          Example: \"International companies are flocking to El Salvador, drawn by favorable government policies that support innovation and economic growth.\"\n",
        "          Keywords: support, international investment, government policies, economic growth, innovation.\n",
        "\n",
        "      ESP4: El Salvador at the Forefront of Economic Modernization and Cryptocurrency Adoption.\n",
        "          Example: \"El Salvador’s adoption of Bitcoin is positioning the country as a pioneer in financial innovation, paving the way for economic modernization.\"\n",
        "          Keywords: cryptocurrency, Bitcoin, economic modernization, innovation, financial pioneer.\n",
        "\n",
        "      ESP5: Bitcoin Policy Opening Doors for Innovation and Attracting Entrepreneurs and International Investors.\n",
        "          Example: \"El Salvador’s Bitcoin policy is attracting a new wave of entrepreneurs and international investors, driving technological innovation and boosting the economy.\"\n",
        "          Keywords: Bitcoin policy, innovation, entrepreneurship, international investors, economic boost.\n",
        "\n",
        "      ESP6: The Bitcoin Revolution Could Break the Country’s Dependence on Traditional Financial Systems.\n",
        "          Example: \"By embracing Bitcoin, El Salvador is reducing its reliance on traditional financial systems and promoting financial independence.\"\n",
        "          Keywords: Bitcoin revolution, financial independence, traditional systems, innovation.\n",
        "\n",
        "      ESP7: El Salvador Affirms Sovereignty and Emerges as a Regional Leader by Standing up to Foreign Pressure.\n",
        "          Example: \"El Salvador is asserting its sovereignty, showing resilience in the face of foreign pressure, and positioning itself as a leader in Central America.\"\n",
        "          Keywords: sovereignty, leadership, resilience, foreign pressure, regional influence.\n",
        "\n",
        "      ESP8: Independent Policies Lead to a New Era of Self-Sufficiency and Greater Regional Influence.\n",
        "          Example: \"The government's independent policies are fostering a new era of self-sufficiency, boosting El Salvador's regional influence.\"\n",
        "          Keywords: independent policies, self-sufficiency, regional influence, leadership, progress.\n",
        "\n",
        "      ESP9: Alternative Media Providing Truth Suppressed by Traditional Media.\n",
        "          Example: \"Alternative media outlets in El Salvador are giving voice to suppressed truths, countering narratives controlled by traditional media.\"\n",
        "          Keywords: alternative media, suppressed truth, transparency, traditional media.\n",
        "\n",
        "      ESP10: Government Promoting Transparency Through its Own Media Channels.\n",
        "          Example: \"The government’s media channels are promoting transparency by delivering unfiltered information directly to the public.\"\n",
        "          Keywords: transparency, government media, unfiltered information, public access, direct communication.\n",
        "\n",
        "      ESP11: Economic Growth Promised by Government Reforms and Innovation.\n",
        "          Example: \"Thanks to recent reforms and a focus on innovation, El Salvador is on the path to sustained economic growth.\"\n",
        "          Keywords: economic growth, reforms, innovation, future prosperity, development.\n",
        "\n",
        "      ESP12: Government Promoting Equality Policies and Empowering Women.\n",
        "          Example: \"New government policies aimed at promoting gender equality are empowering women across different sectors of society.\"\n",
        "          Keywords: equality, women’s rights, empowerment, government policies, gender equality.\n",
        "\n",
        "      ESP13: Support Programs for Vulnerable Women Reinforcing Commitment to Gender Equality.\n",
        "          Example: \"The government’s support programs for women in vulnerable situations are reinforcing its commitment to gender equality and social justice.\"\n",
        "          Keywords: support programs, vulnerable women, gender equality, empowerment, social justice.\n",
        "\n",
        "      ESP14: Policies for LGBT Inclusion and Respect for Rights.\n",
        "          Example: \"El Salvador is leading the way in LGBT inclusion, with new policies that protect and respect the rights of the LGBT community.\"\n",
        "          Keywords: LGBT inclusion, rights, respect, protection, government policies.\n",
        "\n",
        "      ESP15: Government Promoting Tolerance and Combatting Discrimination.\n",
        "          Example: \"Efforts to promote tolerance and fight discrimination are creating a more inclusive society in El Salvador.\"\n",
        "          Keywords: tolerance, anti-discrimination, inclusivity, social progress, government efforts.\n",
        "\n",
        "      ESP16: Strengthening Institutions and Creating Transparency Mechanisms to Combat Corruption.\n",
        "          Example: \"The government has strengthened its institutions and implemented transparency mechanisms to combat corruption, ensuring accountability at all levels.\"\n",
        "          Keywords: strengthened institutions, transparency, anti-corruption, accountability, reforms.\n",
        "\n",
        "      ESP17: Public Support for Anti-Corruption Initiatives Increasing Trust in Institutions.\n",
        "          Example: \"El Salvador's anti-corruption efforts are gaining widespread public support, restoring trust in governmental institutions.\"\n",
        "          Keywords: anti-corruption, public support, institutional trust, transparency, government integrity.\n",
        "\n",
        "    Instructions for Classification:\n",
        "\n",
        "    1- Read carefully the input {input_text}.\n",
        "    2- Determine which narrative(s) it supports based on the content and sentiment expressed.\n",
        "    3- Generate a Json structure:\n",
        "              (\n",
        "                \"classification\": [\"ESP1\", \"ESP2\", \"ESP3\", \"ESP4\", \"ESP5\", \"ESP6\", \"ESP7\", \"ESP8\", \"ESP9\", \"ESP10\", \"ESP11\", \"ESP12\", \"ESP13\", \"ESP14\", \"ESP15\",\"ESP16\", \"ESP17\"]\n",
        "                 at most 2 categories,\n",
        "                \"reasoning\": reasoning of the answer in maximum 50 words.\n",
        "              ).\n",
        "        \"\"\"\n",
        "        self.prompt_ES_agent = PromptTemplate(template=self.ES_agent, input_variables=[\"input_text\"])\n",
        "\n",
        "        self.llm_chain_ES_agent = LLMChain(prompt=self.prompt_ES_agent, llm=self.client)\n",
        "\n",
        "    def _run_agent(self, input_text):\n",
        "        try:\n",
        "            ES_output = self.llm_chain_ES_agent.run({\"input_text\": input_text})\n",
        "            return ES_output\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"Error ES Positive agent\"\n",
        "\n",
        "\n",
        "# 2. Negative Sentiment Narratives\n",
        "class ES_Agent_Negative(WrapperFrame_v1):\n",
        "    def __init__(self, client):\n",
        "        super().__init__(client)\n",
        "\n",
        "        self.ES_agent = \"\"\"\n",
        "    Your task is to analyze and categorize a social media post {input_text} from El salvador according to predefined narrative themes that reflect positive sentiment with validated context.\n",
        "\n",
        "\n",
        "    Narratives to Detect:\n",
        "\n",
        "      ESN1: The Government’s Rigid Approach is Compromising Civil Liberties.\n",
        "      Example: \"The government’s strict security policies are being criticized for compromising civil liberties under the guise of law and order.\"\n",
        "      Keywords: civil liberties, law and order, government rigidity, criticism, human rights.\n",
        "\n",
        "      ESN2: Security Policies Lean Towards Authoritarianism, Risking Escalation of Violence.\n",
        "          Example: \"El Salvador’s security measures are showing authoritarian tendencies, increasing concerns that violence could escalate instead of being reduced.\"\n",
        "          Keywords: authoritarianism, security policies, violence, civil unrest, escalation.\n",
        "\n",
        "      ESN3: Government Actions Attracting International Scrutiny, Deterring Investment.\n",
        "          Example: \"El Salvador is facing international scrutiny over its policies, and there are concerns that foreign investment may be deterred.\"\n",
        "          Keywords: international scrutiny, government policies, deterred investment, foreign relations, economic risk.\n",
        "\n",
        "      ESN4: Cryptocurrency Policies are Risky, Endangering Economic Stability.\n",
        "          Example: \"El Salvador’s decision to adopt Bitcoin is seen as risky, potentially threatening the country’s economic stability due to speculative investments.\"\n",
        "          Keywords: cryptocurrency, Bitcoin, economic risk, speculative investments, financial instability.\n",
        "\n",
        "      ESN5: Bitcoin Policy Isolating the Country from Traditional Financial Allies.\n",
        "          Example: \"The Bitcoin policy is creating uncertainty, as El Salvador becomes more isolated from its traditional financial partners.\"\n",
        "          Keywords: Bitcoin policy, financial isolation, uncertainty, traditional finance, international business.\n",
        "\n",
        "      ESN6: Push for Bitcoin Deepens Economic Vulnerability.\n",
        "          Example: \"The push for Bitcoin could deepen El Salvador's economic vulnerability by increasing dependence on volatile digital assets.\"\n",
        "          Keywords: Bitcoin, economic vulnerability, dependence, volatility, financial instability.\n",
        "\n",
        "      ESN7: El Salvador’s Distancing from International Partners is Alienating Allies.\n",
        "          Example: \"El Salvador’s foreign policies are alienating its regional allies, weakening its influence on the international stage.\"\n",
        "          Keywords: foreign policy, international relations, alienation, regional allies, diminished influence.\n",
        "\n",
        "      ESN8: Independent Policies Increase Isolation from Key Support Networks.\n",
        "          Example: \"By pursuing independent policies, El Salvador is detaching itself from vital support networks, increasing its international isolation.\"\n",
        "          Keywords: independent policies, isolation, support networks, foreign relations, government strategy.\n",
        "\n",
        "      ESN9: Alternative Media is Promoting Polarizing Narratives.\n",
        "          Example: \"Some alternative media outlets in El Salvador are fostering polarizing narratives, raising questions about the reliability of the information they provide.\"\n",
        "          Keywords: alternative media, polarizing narrative, reliability, media trust, information control.\n",
        "\n",
        "      ESN10: Government-Controlled Media Reduces Transparency.\n",
        "          Example: \"By controlling its own media channels, the government may be reducing transparency and limiting access to independent sources of information.\"\n",
        "          Keywords: government control, media, reduced transparency, limited information, censorship concerns.\n",
        "\n",
        "      ESN11: Government Reforms are Creating Economic Uncertainty.\n",
        "          Example: \"Recent government reforms have created uncertainty in the market, posing high risks that threaten the country’s long-term stability.\"\n",
        "          Keywords: government reforms, economic uncertainty, high risk, market instability, future concerns.\n",
        "\n",
        "      ESN12: Insufficient Gender Policies Allow Discrimination Against Women to Persist.\n",
        "          Example: \"Despite reforms, violence and discrimination against women continue, showing that the government's gender policies are insufficient.\"\n",
        "          Keywords: gender policies, insufficient reform, violence against women, discrimination, government failure.\n",
        "\n",
        "      ESN13: Lack of Funding Weakens Support Programs for Women’s Rights.\n",
        "          Example: \"Government programs aimed at protecting women’s rights are underfunded, severely limiting their effectiveness.\"\n",
        "          Keywords: women’s rights, underfunding, lack of support, program effectiveness, gender equality.\n",
        "\n",
        "      ESN14: Lack of Legal Protection for the LGBT Community Increases Vulnerability.\n",
        "          Example: \"Without strong legal protections, the LGBT community in El Salvador faces significant vulnerabilities, limiting their full participation in society.\"\n",
        "          Keywords: LGBT rights, legal protection, vulnerability, exclusion, government policy.\n",
        "\n",
        "      ESN15: Government Criticized for Failing to Actively Promote LGBT Rights.\n",
        "          Example: \"Current government policies have been criticized for not doing enough to actively promote the rights of the LGBT community, leading to an atmosphere of exclusion.\"\n",
        "          Keywords: LGBT rights, government inaction, exclusion, criticism, equality.\n",
        "\n",
        "      ESN16: Corruption Allegations Weaken Public Trust.\n",
        "          Example: \"Persistent corruption allegations against government officials are eroding public trust and undermining the effectiveness of institutions.\"\n",
        "          Keywords: corruption, public trust, institutional effectiveness, government officials, allegations.\n",
        "\n",
        "      ESN17: Lack of Transparency Raises Concerns About Government Integrity.\n",
        "          Example: \"The lack of transparency in government processes is raising concerns about the integrity and accountability of public management.\"\n",
        "          Keywords: transparency, government integrity, accountability, concerns, public management.\n",
        "\n",
        "    Instructions for Classification:\n",
        "\n",
        "    1- Read carefully the input {input_text}.\n",
        "    2- Determine which narrative(s) it supports based on the content and sentiment expressed.\n",
        "    3- Generate a Json structure:\n",
        "              (\n",
        "                \"classification\": [\"ESN1\", \"ESN2\", \"ESN3\", \"ESN4\", \"ESN5\", \"ESN6\", \"ESN7\", \"ESN8\", \"ESN9\", \"ESN10\", \"ESN11\", \"ESN12\", \"ESN13\", \"ESN14\", \"ESN15\",\"ESN16\",\"ESN17\"]\n",
        "                 at most 2 categories,\n",
        "                \"reasoning\": reasoning of the answer in maximum 50 words.\n",
        "              ).\n",
        "        \"\"\"\n",
        "        self.prompt_ES_agent = PromptTemplate(template=self.ES_agent, input_variables=[\"input_text\"])\n",
        "\n",
        "        self.llm_chain_ES_agent = LLMChain(prompt=self.prompt_ES_agent, llm=self.client)\n",
        "\n",
        "    def _run_agent(self, input_text):\n",
        "        try:\n",
        "            ES_output = self.llm_chain_ES_agent.run({\"input_text\": input_text})\n",
        "            return ES_output\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"Error ES Negative agent\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Intention dictionary to map intention codes to human-readable intentions\n",
        "INTENTION_DICT = {\n",
        "    # Positive Sentiment Narratives (ESP)\n",
        "    \"ESP1\": \"The Government’s Firm Approach Restoring Law and Order\",\n",
        "    \"ESP2\": \"El Salvador is Becoming a Model of Security and Strong Leadership in Fighting Gang Violence\",\n",
        "    \"ESP3\": \"Government Policies Attracting Support and International Investment\",\n",
        "    \"ESP4\": \"El Salvador at the Forefront of Economic Modernization and Cryptocurrency Adoption\",\n",
        "    \"ESP5\": \"Bitcoin Policy Opening Doors for Innovation and Attracting Entrepreneurs and International Investors\",\n",
        "    \"ESP6\": \"The Bitcoin Revolution Could Break the Country’s Dependence on Traditional Financial Systems\",\n",
        "    \"ESP7\": \"El Salvador Affirms Sovereignty and Emerges as a Regional Leader by Standing up to Foreign Pressure\",\n",
        "    \"ESP8\": \"Independent Policies Lead to a New Era of Self-Sufficiency and Greater Regional Influence\",\n",
        "    \"ESP9\": \"Alternative Media Providing Truth Suppressed by Traditional Media\",\n",
        "    \"ESP10\": \"Government Promoting Transparency Through its Own Media Channels\",\n",
        "    \"ESP11\": \"Economic Growth Promised by Government Reforms and Innovation\",\n",
        "    \"ESP12\": \"Government Promoting Equality Policies and Empowering Women\",\n",
        "    \"ESP13\": \"Support Programs for Vulnerable Women Reinforcing Commitment to Gender Equality\",\n",
        "    \"ESP14\": \"Policies for LGBT Inclusion and Respect for Rights\",\n",
        "    \"ESP15\": \"Government Promoting Tolerance and Combatting Discrimination\",\n",
        "    \"ESP16\": \"Strengthening Institutions and Creating Transparency Mechanisms to Combat Corruption\",\n",
        "    \"ESP17\": \"Public Support for Anti-Corruption Initiatives Increasing Trust in Institutions\",\n",
        "\n",
        "    # Negative Sentiment Narratives (ESN)\n",
        "    \"ESN1\": \"The Government’s Rigid Approach is Compromising Civil Liberties\",\n",
        "    \"ESN2\": \"Security Policies Lean Towards Authoritarianism, Risking Escalation of Violence\",\n",
        "    \"ESN3\": \"Government Actions Attracting International Scrutiny, Deterring Investment\",\n",
        "    \"ESN4\": \"Cryptocurrency Policies are Risky, Endangering Economic Stability\",\n",
        "    \"ESN5\": \"Bitcoin Policy Isolating the Country from Traditional Financial Allies\",\n",
        "    \"ESN6\": \"Push for Bitcoin Deepens Economic Vulnerability\",\n",
        "    \"ESN7\": \"El Salvador’s Distancing from International Partners is Alienating Allies\",\n",
        "    \"ESN8\": \"Independent Policies Increase Isolation from Key Support Networks\",\n",
        "    \"ESN9\": \"Alternative Media is Promoting Polarizing Narratives\",\n",
        "    \"ESN10\": \"Government-Controlled Media Reduces Transparency\",\n",
        "    \"ESN11\": \"Government Reforms are Creating Economic Uncertainty\",\n",
        "    \"ESN12\": \"Insufficient Gender Policies Allow Discrimination Against Women to Persist\",\n",
        "    \"ESN13\": \"Lack of Funding Weakens Support Programs for Women’s Rights\",\n",
        "    \"ESN14\": \"Lack of Legal Protection for the LGBT Community Increases Vulnerability\",\n",
        "    \"ESN15\": \"Government Criticized for Failing to Actively Promote LGBT Rights\",\n",
        "    \"ESN16\": \"Corruption Allegations Weaken Public Trust\",\n",
        "    \"ESN17\": \"Lack of Transparency Raises Concerns About Government Integrity\"\n",
        "}\n",
        "\n",
        "\n",
        "# Build Agent Framework\n",
        "class ClassAgent():\n",
        "    def __init__(self, llm_35):\n",
        "        self.llm_35 = llm_35\n",
        "        self.class_agent = \"\"\" You are an agent with the task of analyzing a tweet input {input} from El Salvador.\n",
        "        you are tasked with identifying entities mentioned or refered to in the input and generate and optimized query to launch a websearch\n",
        "        concerning the input.\n",
        "\n",
        "        optimal web query.\n",
        "\n",
        "        Instructions:\n",
        "\n",
        "            1- Read the input carefully.\n",
        "\n",
        "            2- Indentify the real entities in it (actors or institutions from El salvador).\n",
        "\n",
        "            3-Segment the original input depending on the entity they refer to.\n",
        "\n",
        "            4- Reframe the tweet for an optimal websearch in Spanish.\n",
        "\n",
        "            5- Generate a Json output following this pattern:\n",
        "\n",
        "            (\n",
        "\n",
        "            \"entities\": [Indentify the real entities in it (actors or institutions from El salvador)in the tweet as a list],\n",
        "            \"segments\":[segments of the original input divided by the entity refered to]\n",
        "            \"search\": Reframe the tweet for an optimal websearch in Spanish,\n",
        "            )\n",
        "            \"\"\"\n",
        "        self.prompt_class_agent = PromptTemplate(template=self.class_agent, input_variables=[\"input\"])\n",
        "\n",
        "        # Use LLMChain here with the knowledge it may be deprecated\n",
        "        self.llm_chain_class_agent = LLMChain(prompt=self.prompt_class_agent, llm=self.llm_35)\n",
        "\n",
        "    def _run_class_branch(self, input):\n",
        "        try:\n",
        "            class_agent_output = self.llm_chain_class_agent.run({\"input\": input})\n",
        "            return class_agent_output\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"Error class layer\"\n",
        "\n",
        "\n",
        "class SentimentAgent():\n",
        "    def __init__(self, llm_35):\n",
        "        self.llm_35 = llm_35\n",
        "\n",
        "        self.sentiment_agent = \"\"\"\n",
        "      You are an expert agent that has to analyze and classify a tweet {input} from El Salvador according to its sentiment.\n",
        "      Your job is to categorize each tweet into one of three categories based on its sentiment: 1 (positive) or -1 (negative).\n",
        "\n",
        "\n",
        "\n",
        "      Guidelines:\n",
        "      . 1 (Positive): The tweet expresses a positive sentiment, such as happiness, praise, or admiration.\n",
        "      .-1 (Negative): The tweet expresses a negative sentiment, such as criticism, disappointment, or disapproval.\n",
        "\n",
        "        Instructions:\n",
        "\n",
        "            - Read the input carefully.\n",
        "\n",
        "            - Analize the context\n",
        "\n",
        "            - Determine the sentiment expressed based on the content taking into consideration the context is El Salvador.\n",
        "\n",
        "            - Generate a Json output following this pattern:\n",
        "\n",
        "                    (\n",
        "                      \"label\": 1,-1.\n",
        "                      \"reasoning: reasoning of the answer in 50 words.\n",
        "                    ).\n",
        "        \"\"\"\n",
        "        self.prompt_sentiment_agent = PromptTemplate(template=self.sentiment_agent, input_variables=[\"input\"])\n",
        "\n",
        "        # Use LLMChain\n",
        "        self.llm_chain_sentiment_agent = LLMChain(prompt=self.prompt_sentiment_agent, llm=self.llm_35)\n",
        "\n",
        "    def _run_sentiment_branch(self, input):\n",
        "        try:\n",
        "            sentiment_agent_output = self.llm_chain_sentiment_agent.run({'input': input})\n",
        "            return sentiment_agent_output\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"Error sentiment layer\"\n",
        "\n",
        "class ContextBuilderAgent():\n",
        "    def __init__(self, llm_35):\n",
        "        self.llm_35 = llm_35\n",
        "\n",
        "        self.context_builder_agent = \"\"\"you are an agent with the task of creating a context from a social media input {input} from el Salvador.\n",
        "        you are provided with the result of a web {search_output},\n",
        "\n",
        "        your job is to :\n",
        "        1- Review the input {input} and search output {search_output},\n",
        "        2- Verify all the entries of the search_output {search_output} one by one and analize the aligment with the input {input},\n",
        "        3- Build a context from the search_output to help understand the context of the input\n",
        "        4-Provide a json output :\n",
        "        (\n",
        "        \"context\": Context to help understand the input.\n",
        "         )\n",
        "\n",
        "      just provide a json output\n",
        "        \"\"\"\n",
        "        self.prompt_context_builder_agent = PromptTemplate(template=self.context_builder_agent, input_variables=[\"input\",\"search_output\"])\n",
        "\n",
        "        # Corrected the LLMChain instantiation\n",
        "        self.llm_chain_context_agent = LLMChain(prompt=self.prompt_context_builder_agent, llm=self.llm_35)\n",
        "\n",
        "    def _run_context_builder_branch(self,input,search_output):\n",
        "        self.input=input\n",
        "        self.search_output=search_output\n",
        "\n",
        "        try:\n",
        "            context_agent_builder_output = self.llm_chain_context_agent.run({'input':self.input,'search_output':self.search_output})\n",
        "            return context_agent_builder_output\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"Error context building layer\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Class to define the overall processing pipeline\n",
        "class VerificationPipeline:\n",
        "    def __init__(self, llm_35, llm_4, input):\n",
        "        self.class_agent = ClassAgent(llm_35)\n",
        "        self.sentiment_agent = SentimentAgent(llm_35)\n",
        "        #self.context_building_agent = ContextBuilderAgent(llm_35)\n",
        "        self.llm_4 = llm_4\n",
        "        self.llm_35 = llm_35\n",
        "        self.input = input\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def process_news(self):\n",
        "        self.logger.info('Process Initialized')\n",
        "\n",
        "        # Entity extraction and search classification\n",
        "        classification_result = self.classify_news(self.input)\n",
        "        if not classification_result or 'error' in classification_result:\n",
        "            self.logger.error(f\"Classification failed: {classification_result}\")\n",
        "            return classification_result\n",
        "\n",
        "        # Extract entities and segments\n",
        "        entities = classification_result.get(\"entities\", [])\n",
        "        segments = classification_result.get(\"segments\", [])\n",
        "        search_input=classification_result.get(\"search\", [])\n",
        "        print(\"search\",search_input)\n",
        "        #search_output = info_extraction(search_input)\n",
        "        # Context Building\n",
        "        #context_build_result = self.build_context(self.input, search_output)\n",
        "        #if 'error' in context_build_result:\n",
        "            #return context_build_result\n",
        "\n",
        "        #context = context_build_result.get(\"context\", [])\n",
        "        # Initialize final output container\n",
        "        final_output = []\n",
        "\n",
        "        # Loop through each entity and its corresponding segment\n",
        "        for entity, segment in zip(entities, segments):\n",
        "            self.logger.info(f\"Processing entity: {entity}\")\n",
        "\n",
        "            # Sentiment analysis for the segment\n",
        "            sentiment_analysis_result = self.analyze_sentiment(segment)\n",
        "            if 'error' in sentiment_analysis_result:\n",
        "                return sentiment_analysis_result\n",
        "            print('Sentiment',sentiment_analysis_result)\n",
        "            # Convert sentiment label (1 -> Positive, -1 -> Negative)\n",
        "            sentiment_label = sentiment_analysis_result['label']\n",
        "            sentiment_description = \"Positive\" if sentiment_label == 1 else \"Negative\"\n",
        "\n",
        "            # Determine narrative intention (positive or negative)\n",
        "            intention_result = self.determine_intention(sentiment_label, segment)\n",
        "            if 'error' in intention_result:\n",
        "                return intention_result\n",
        "            print('Intent',intention_result )\n",
        "            # Get narrative descriptions based on classification\n",
        "            narrative_descriptions = [INTENTION_DICT.get(code, \"Unknown Narrative\") for code in intention_result['classification']]\n",
        "\n",
        "            # Collect results for the current entity/segment\n",
        "            entity_output = {\n",
        "                \"entity\": entity,\n",
        "                \"segment\": segment,\n",
        "                \"sentiment_label\": sentiment_label,\n",
        "                \"sentiment_description\": sentiment_description,\n",
        "                \"intention_classification\": intention_result['classification'],\n",
        "                \"narrative_descriptions\": narrative_descriptions,\n",
        "                \"reasoning\": intention_result['reasoning'],\n",
        "            }\n",
        "\n",
        "            final_output.append(entity_output)\n",
        "\n",
        "        # Return the aggregated results for all entities\n",
        "        return final_output\n",
        "\n",
        "    def classify_news(self, input):\n",
        "        # Parsing output from class_agent\n",
        "        result = self.class_agent._run_class_branch(input)\n",
        "        print(f\"ClassAgent output: {result}\")  # Debugging: check raw output\n",
        "\n",
        "        try:\n",
        "            data = json.loads(result)  # Ensure the result is parsed as a dictionary\n",
        "            return data\n",
        "        except json.JSONDecodeError:\n",
        "            self.logger.error(f\"Error decoding classification result: {result}\")\n",
        "            return {\"error\": \"Failed to classify the news.\"}\n",
        "\n",
        "    def analyze_sentiment(self, input):\n",
        "        # Parsing output from sentiment_agent\n",
        "        result = self.sentiment_agent._run_sentiment_branch(input)\n",
        "        try:\n",
        "            data = json.loads(result)  # Ensure the result is parsed as a dictionary\n",
        "            return data\n",
        "        except json.JSONDecodeError:\n",
        "            self.logger.error(f\"Error decoding sentiment result: {result}\")\n",
        "            return {\"error\": \"Failed to analyze sentiment.\"}\n",
        "\n",
        "    def determine_intention(self, sentiment_label, input_text):\n",
        "      # Select the appropriate agent based on sentiment\n",
        "      if sentiment_label == 1:\n",
        "          agent = ES_Agent_Positive(self.llm_35)\n",
        "      elif sentiment_label == -1:\n",
        "          agent = ES_Agent_Negative(self.llm_35)\n",
        "      else:\n",
        "          return {\"error\": \"Unexpected sentiment label\"}\n",
        "\n",
        "      # Run the agent to get the intention\n",
        "      result = agent._run_agent(input_text)\n",
        "\n",
        "      # Check if the result is empty or not a valid string\n",
        "      if not result or result.strip() == \"\":\n",
        "          self.logger.error(f\"Agent returned an empty result: {result}\")\n",
        "          return {\"error\": \"Agent returned an empty result\"}\n",
        "\n",
        "      try:\n",
        "          # If result is already a dict, no need to decode\n",
        "          if isinstance(result, dict):\n",
        "              return result\n",
        "\n",
        "          # Check if it's a string and then decode it\n",
        "          if isinstance(result, str):\n",
        "              # Ensure it's a valid JSON string\n",
        "              data = json.loads(result)\n",
        "              return data\n",
        "\n",
        "      except json.JSONDecodeError as e:\n",
        "          # Log detailed error if the JSON is not properly formatted\n",
        "          self.logger.error(f\"Error decoding intention result: {result}, Error: {e}\")\n",
        "          return {\"error\": \"Failed to determine the intention due to invalid JSON.\"}\n",
        "\n",
        "      except Exception as e:\n",
        "          # Catch any other unexpected error\n",
        "          self.logger.error(f\"Unexpected error: {e}\")\n",
        "          return {\"error\": \"Failed to determine the intention due to an unexpected error.\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_base_domain(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "    path = parsed_url.path\n",
        "\n",
        "    if domain.startswith(\"www.\"):\n",
        "        domain = domain[4:]\n",
        "\n",
        "    # Combine domain and the first part of the path for social media filtering\n",
        "    if \"facebook.com\" in domain or \"instagram.com\" in domain or \"twitter.com\" in domain or \"x.com\" in domain:\n",
        "        return domain + path.split('/')[1] + '/'\n",
        "\n",
        "    return domain\n",
        "\n",
        "def info_extraction(subject, length=500, api_key=serper_api_key, min_search=30):\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "\n",
        "    payload = json.dumps({\n",
        "        \"q\": subject,\n",
        "        \"gl\": \"sv\",\n",
        "        \"hl\": \"es-419\",\n",
        "        \"num\": min_search,\n",
        "        \"tbs\": 'qdr:w',\n",
        "    })\n",
        "\n",
        "    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "        organic_results = results.get(\"organic\", [])\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error during search: {e}\")\n",
        "        return [], [], []\n",
        "\n",
        "    all_media_outlets, reliable_media, unreliable_media = [], [], []\n",
        "\n",
        "    for result in organic_results:\n",
        "        snippet = result.get('snippet')\n",
        "        source_url = result.get('link')\n",
        "        if snippet and source_url:\n",
        "            base_domain = extract_base_domain(source_url)\n",
        "            all_media_outlets.append(base_domain)\n",
        "\n",
        "            if base_domain in RELIABLE_SOURCES:\n",
        "                reliability_index = RELIABLE_SOURCES[base_domain]\n",
        "                reliable_media.append({\n",
        "                    \"snippet\": snippet[:length],\n",
        "                    \"source\": source_url,  # Include the full link\n",
        "                    \"reliability_index\": reliability_index\n",
        "                })\n",
        "            else:\n",
        "                unreliable_media.append({\n",
        "                    \"snippet\": snippet[:length],\n",
        "                    \"source\": source_url  # Include the full link\n",
        "                })\n",
        "        else:\n",
        "            print(f\"Missing data in results: {result}\")\n",
        "\n",
        "    sorted_reliable = sorted(reliable_media, key=lambda x: x['reliability_index'], reverse=True)\n",
        "\n",
        "    return all_media_outlets, sorted_reliable, unreliable_media\n",
        "\n",
        "    def build_context(self, input, search_output):\n",
        "        try:\n",
        "            result = self.context_building_agent._run_context_builder_branch(input, search_output)\n",
        "            self.logger.info(result)\n",
        "            data = json.loads(result)\n",
        "            context=data['context']\n",
        "            return context\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during context building: {e}\")\n",
        "            return {\"error\": \"Failed to build context.\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VupzkbOJOMa1",
        "outputId": "debf6dfe-53e5-49ca-fe79-622565152a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5393687158e9>:290: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  self.llm_chain_class_agent = LLMChain(prompt=self.prompt_class_agent, llm=self.llm_35)\n",
            "<ipython-input-7-5393687158e9>:294: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  class_agent_output = self.llm_chain_class_agent.run({\"input\": input})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassAgent output: \n",
            "{\n",
            "\"entities\": [\"grupos terroristas\", \"oposicion\"],\n",
            "\"segments\": [\n",
            "\"Hemos dado otro golpe contundente a la jerarquía de estos grupos terroristas mientras\",\n",
            "\"la oposicion se dedica a sabotear nuestro progreso.\"\n",
            "],\n",
            "\"search\": \"Golpe contundente a grupos terroristas El Salvador oposicion sabotear progreso\"\n",
            "}\n",
            "search Golpe contundente a grupos terroristas El Salvador oposicion sabotear progreso\n",
            "Sentiment {'label': 1, 'reasoning': 'The tweet expresses a positive sentiment as it highlights a successful strike against terrorist groups, showing progress and effectiveness in combating them. This can be seen as a positive development for the safety and security of the region.'}\n",
            "Intent {'classification': ['ESP7', 'ESP16'], 'reasoning': \"The post highlights El Salvador's success in combating terrorist groups, affirming sovereignty and resilience against foreign pressure (ESP7). It also implies a firm approach and commitment to law and order, potentially involving transparency mechanisms to combat corruption (ESP16).\"}\n",
            "Sentiment {'label': -1, 'reasoning': 'The tweet expresses a negative sentiment as it criticizes the opposition for sabotaging progress in El Salvador. The tone is disapproving and implies frustration towards the actions of the opposition.'}\n",
            "Intent {'classification': ['ESN1', 'ESN16'], 'reasoning': 'The post suggests that the opposition is sabotaging progress, indicating a potential compromise of civil liberties by the government (ESN1). Additionally, it implies corruption allegations, which weaken public trust (ESN16).'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'grupos terroristas',\n",
              "  'segment': 'Hemos dado otro golpe contundente a la jerarquía de estos grupos terroristas mientras',\n",
              "  'sentiment_label': 1,\n",
              "  'sentiment_description': 'Positive',\n",
              "  'intention_classification': ['ESP7', 'ESP16'],\n",
              "  'narrative_descriptions': ['El Salvador Affirms Sovereignty and Emerges as a Regional Leader by Standing up to Foreign Pressure',\n",
              "   'Strengthening Institutions and Creating Transparency Mechanisms to Combat Corruption'],\n",
              "  'reasoning': \"The post highlights El Salvador's success in combating terrorist groups, affirming sovereignty and resilience against foreign pressure (ESP7). It also implies a firm approach and commitment to law and order, potentially involving transparency mechanisms to combat corruption (ESP16).\"},\n",
              " {'entity': 'oposicion',\n",
              "  'segment': 'la oposicion se dedica a sabotear nuestro progreso.',\n",
              "  'sentiment_label': -1,\n",
              "  'sentiment_description': 'Negative',\n",
              "  'intention_classification': ['ESN1', 'ESN16'],\n",
              "  'narrative_descriptions': ['The Government’s Rigid Approach is Compromising Civil Liberties',\n",
              "   'Corruption Allegations Weaken Public Trust'],\n",
              "  'reasoning': 'The post suggests that the opposition is sabotaging progress, indicating a potential compromise of civil liberties by the government (ESN1). Additionally, it implies corruption allegations, which weaken public trust (ESN16).'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Example execution\n",
        "input = \"Hemos dado otro golpe contundente a la jerarquía de estos grupos terroristas mientras la oposicion se dedica a sabotear nuestro progreso.\"\n",
        "verification_process = VerificationPipeline(llm_35, llm_4, input)\n",
        "result = verification_process.process_news()\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}